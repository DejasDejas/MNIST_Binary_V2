{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Omniglot_binary_classification.ipynb","provenance":[],"authorship_tag":"ABX9TyMij16HPVJvnulNMlfvVOea"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"omBCpg_mpgmJ","colab_type":"text"},"source":["# Mount Drive"]},{"cell_type":"code","metadata":{"id":"6xc5nKj1pP8o","colab_type":"code","outputId":"f8ef4e3b-b7bd-48d9-f11d-32490d2f2bae","executionInfo":{"status":"ok","timestamp":1586771194604,"user_tz":-120,"elapsed":865,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["#Import drive\n","from google.colab import drive\n","#Mount Google Drive\n","drive.mount(\"/content/drive\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XDVpM_iOpcYM","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('drive/My Drive/Work/Thesis_Julien_Dejasmin/Work/code/Binary_activations_V2/MNIST_Binary_V2')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rcunyCQHpitd","colab_type":"code","outputId":"f5b49df8-8f25-4476-9456-b4109a6d165f","executionInfo":{"status":"ok","timestamp":1586770683079,"user_tz":-120,"elapsed":15291,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}},"colab":{"base_uri":"https://localhost:8080/","height":66}},"source":["!ls"],"execution_count":5,"outputs":[{"output_type":"stream","text":["config.py   distributions  README.md\t     trained_models\n","data\t    experiments    requirements.txt  utils\n","DataLoader  __pycache__    results\t     visualize\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AVdxLztRpksL","colab_type":"text"},"source":["# Import:"]},{"cell_type":"code","metadata":{"id":"BUid6ah0pl09","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","from torch import load\n","import torch\n","# from torchsummary import summary\n","\n","from DataLoader.dataLoaders import get_omniglot_dataloaders_v1\n","from utils.models import NoBinaryNetOmniglotClassification, BinaryNetOmniglotClassification\n","from utils.training import training, test, gpu_config\n","\n","import collections\n","import torch.nn as nn\n","from functools import partial\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# for visualization\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.transforms.functional as F\n","from torchvision import datasets, transforms\n","from PIL import Image\n","from visualize.viz import show_som_examples"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"huavDxQOpl9O","colab_type":"text"},"source":["# Parameters:"]},{"cell_type":"code","metadata":{"id":"nlkoYNu_pjgu","colab_type":"code","colab":{}},"source":["# parameters default values\n","lr = 1e-3\n","# momentum = 0.9\n","nb_epoch = 50\n","batch_size_train = 64\n","batch_size_test = 128"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9wKZzZBip9Jp","colab_type":"text"},"source":["# Dataset:"]},{"cell_type":"code","metadata":{"id":"Qo1tbdXUqel6","colab_type":"code","outputId":"7e8b2ebf-dc35-457a-9aa4-e07205bd8987","executionInfo":{"status":"ok","timestamp":1586771208193,"user_tz":-120,"elapsed":7898,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}},"colab":{"base_uri":"https://localhost:8080/","height":66}},"source":["# Omniglto Dataset\n","train_loader, test_loader = get_omniglot_dataloaders_v1(batch_size_train, batch_size_test)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Number of training examples: 272\n","Number of testing examples: 16\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FnoWIHotp2MN","colab_type":"text"},"source":["# Model:"]},{"cell_type":"code","metadata":{"id":"Vd9v0tI8p1oL","colab_type":"code","colab":{}},"source":["# Model, activation type, estimator type\n","def get_my_model(binary, stochastic, reinforce, first_conv_layer, second_conv_layer, \n","                 third_conv_layer, fourth_conv_layer):\n","\n","    if binary:\n","        if stochastic:\n","            mode = 'Stochastic'\n","            names_model = 'Omniglot_classif_Stochastic'\n","        else:\n","            mode = 'Deterministic'\n","            names_model = 'Omniglot_classif_Deterministic'\n","        if reinforce:\n","            estimator = 'REINFORCE'\n","            names_model += '_REINFORCE'\n","        else:\n","            estimator = 'ST'\n","            names_model += '_ST'\n","        if first_conv_layer:\n","            names_model += '_first_conv_binary'\n","        if second_conv_layer:\n","            names_model += '_second_conv_binary'\n","        if third_conv_layer:\n","            names_model += '_third_conv_binary'\n","        if fourth_conv_layer:\n","            names_model += '_fourth_conv_binary'\n","        model = BinaryNetOmniglotClassification(first_conv_layer=first_conv_layer, second_conv_layer=second_conv_layer,\n","                                        third_conv_layer=third_conv_layer, fourth_conv_layer=fourth_conv_layer,\n","                                        mode=mode, estimator=estimator)\n","    else:\n","        model = NoBinaryNetOmniglotClassification()\n","        names_model = 'Omniglot_classif_NonBinaryNet'\n","        mode = None\n","        estimator = None\n"," \n","    # gpu config:\n","    model, use_gpu = gpu_config(model)\n","    return model, names_model, use_gpu\n","\n","def no_binary_model():\n","  # Parameters\n","  slope_annealing = False\n","  reinforce = False\n","  stochastic = False\n","  binary = False\n","  plot_result = True\n","  first_conv_layer = False\n","  second_conv_layer = False\n","  third_conv_layer = False\n","  fourth_conv_layer = False\n","  omniglot = True\n","\n","  model, name_model, use_gpu = get_my_model(binary, stochastic, reinforce, first_conv_layer, \n","                                  second_conv_layer, third_conv_layer, fourth_conv_layer)\n","\n","  return model, name_model\n","\n","def binary_model_first():\n","  # Parameters\n","  slope_annealing = False\n","  reinforce = False\n","  stochastic = True\n","  binary = True\n","  plot_result = True\n","  first_conv_layer = True\n","  second_conv_layer = False\n","  third_conv_layer = False\n","  fourth_conv_layer = False\n","  omniglot = True\n","\n","  model, name_model, use_gpu = get_my_model(binary, stochastic, reinforce, first_conv_layer, \n","                                  second_conv_layer, third_conv_layer, fourth_conv_layer)\n","  return model, name_model\n","\n","def binary_model_second():\n","  # Parameters\n","  slope_annealing = False\n","  reinforce = False\n","  stochastic = True\n","  binary = True\n","  plot_result = True\n","  first_conv_layer = False\n","  second_conv_layer = True\n","  third_conv_layer = False\n","  fourth_conv_layer = False\n","  omniglot = True\n","\n","  model, name_model, use_gpu = get_my_model(binary, stochastic, reinforce, first_conv_layer, \n","                                  second_conv_layer, third_conv_layer, fourth_conv_layer)\n","  return model, name_model\n","\n","def binary_model_third():\n","  # Parameters\n","  slope_annealing = False\n","  reinforce = False\n","  stochastic = True\n","  binary = True\n","  plot_result = True\n","  first_conv_layer = False\n","  second_conv_layer = False\n","  third_conv_layer = True\n","  fourth_conv_layer = False\n","  omniglot = True\n","\n","  model, name_model, use_gpu = get_my_model(binary, stochastic, reinforce, first_conv_layer, \n","                                  second_conv_layer, third_conv_layer, fourth_conv_layer)\n","  return model, name_model\n","\n","def binary_model_fourth():\n","  # Parameters\n","  slope_annealing = False\n","  reinforce = False\n","  stochastic = True\n","  binary = True\n","  plot_result = True\n","  first_conv_layer = False\n","  second_conv_layer = False\n","  third_conv_layer = False\n","  fourth_conv_layer = True\n","  omniglot = True\n","\n","  model, name_model, use_gpu = get_my_model(binary, stochastic, reinforce, first_conv_layer, \n","                                  second_conv_layer, third_conv_layer, fourth_conv_layer)\n","  return model, name_model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cv1mmxFnquV_","colab_type":"text"},"source":["# Training:"]},{"cell_type":"markdown","metadata":{"id":"AknoDCXolSTM","colab_type":"text"},"source":["## Omniglot classification Network no binary:"]},{"cell_type":"code","metadata":{"id":"lFPkZexUq0OH","colab_type":"code","colab":{}},"source":["model, name_model = no_binary_model()\n","print(name_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6FLFlTKArRDf","colab_type":"code","colab":{}},"source":["print(model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0p_52NebsejY","colab_type":"text"},"source":["### Training:"]},{"cell_type":"code","metadata":{"id":"p6pFWaVKr65o","colab_type":"code","colab":{}},"source":["# optimizer\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","train_loss, train_acc, val_loss, val_acc = training(use_gpu, model, name_model, nb_epoch, train_loader, test_loader,\n","                                                    optimizer, plot_result, slope_annealing)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YRR6KfOFsvQY","colab_type":"text"},"source":["### Test:"]},{"cell_type":"code","metadata":{"id":"_kWFpipmsfMr","colab_type":"code","outputId":"cfb1f289-45df-435d-eee6-f08c09024529","executionInfo":{"status":"ok","timestamp":1586766416762,"user_tz":-120,"elapsed":1297396,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}},"colab":{"base_uri":"https://localhost:8080/","height":135}},"source":["# test\n","model.load_state_dict(load('trained_models/Omniglot_classif/' + name_model + '.pt', map_location=torch.device('cpu')))\n","test_loss, test_acc = test(use_gpu, model, test_loader)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Work/Thesis_Julien_Dejasmin/Work/code/Binary_activations_V2/MNIST_Binary_V2/utils/training.py:133: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(data, volatile=True), Variable(target)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["Test set: Average loss: 0.1696, Accuracy: 1860/1928 (96%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"m7L5dF9Es9Cz","colab_type":"text"},"source":["## Omniglot classification Network with binary first conv layer:"]},{"cell_type":"code","metadata":{"id":"OhaOPZwItHcm","colab_type":"code","outputId":"1d86169b-4ad2-4f02-8f30-1a7f71cd587a","executionInfo":{"status":"ok","timestamp":1586767653143,"user_tz":-120,"elapsed":733,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["model, name_model = binary_model_first()\n","print(name_model)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["no gpu available !\n","Omniglot_classif_Stochastic_ST_first_conv_binary\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uo8qlbyFk0jw","colab_type":"text"},"source":["### Training:"]},{"cell_type":"code","metadata":{"id":"minNoKM-uls3","colab_type":"code","colab":{}},"source":["# optimizer\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","train_loss, train_acc, val_loss, val_acc = training(use_gpu, model, name_model, nb_epoch, train_loader, test_loader,\n","                                                    optimizer, plot_result, slope_annealing)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZMDweu2Bk2IP","colab_type":"text"},"source":["### Test:"]},{"cell_type":"code","metadata":{"id":"fS1m3EVduoqn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":135},"outputId":"ed0379c4-7898-470d-cdac-683e30b27d96","executionInfo":{"status":"ok","timestamp":1586767690339,"user_tz":-120,"elapsed":35653,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}}},"source":["# test\n","model.load_state_dict(load('trained_models/Omniglot_classif/' + name_model + '.pt', map_location=torch.device('cpu')))\n","test_loss, test_acc = test(use_gpu, model, test_loader)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Work/Thesis_Julien_Dejasmin/Work/code/Binary_activations_V2/MNIST_Binary_V2/utils/training.py:133: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(data, volatile=True), Variable(target)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["Test set: Average loss: 0.3937, Accuracy: 1855/1928 (96%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o9771VlblfqH","colab_type":"text"},"source":["## Omniglot classification Network with binary second conv layer:"]},{"cell_type":"code","metadata":{"id":"oyIx7t9VlgAA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"4ba38c80-002f-4025-fa6d-9120dcd9fe80","executionInfo":{"status":"ok","timestamp":1586767690341,"user_tz":-120,"elapsed":33278,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}}},"source":["model, name_model = binary_model_second()\n","print(name_model)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["no gpu available !\n","Omniglot_classif_Stochastic_ST_second_conv_binary\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZipHqXNqmm82","colab_type":"text"},"source":["### Training:"]},{"cell_type":"code","metadata":{"id":"mv6KfY6qmmPU","colab_type":"code","colab":{}},"source":["# optimizer\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","train_loss, train_acc, val_loss, val_acc = training(use_gpu, model, name_model, nb_epoch, train_loader, test_loader,\n","                                                    optimizer, plot_result, slope_annealing)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5shckeRxli0U","colab_type":"text"},"source":["### Test:"]},{"cell_type":"code","metadata":{"id":"G8m7vJVZlkPW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":135},"outputId":"f6eaf3fd-941d-4957-e0ff-7a68a6ab81ef","executionInfo":{"status":"ok","timestamp":1586767722693,"user_tz":-120,"elapsed":32342,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}}},"source":["# test\n","model.load_state_dict(load('trained_models/Omniglot_classif/' + name_model + '.pt', map_location=torch.device('cpu')))\n","test_loss, test_acc = test(use_gpu, model, test_loader)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Work/Thesis_Julien_Dejasmin/Work/code/Binary_activations_V2/MNIST_Binary_V2/utils/training.py:133: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(data, volatile=True), Variable(target)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["Test set: Average loss: 5.1414, Accuracy: 376/1928 (20%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"69zdj-nBk3_e","colab_type":"text"},"source":["## Omniglot classification Network with binary third conv layer:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KgyaBwX-2Xwm","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"1db837b2-4629-4115-bb4b-ba923046eaf0","executionInfo":{"status":"ok","timestamp":1586767722695,"user_tz":-120,"elapsed":32332,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}}},"source":["model, name_model = binary_model_third()\n","print(name_model)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["no gpu available !\n","Omniglot_classif_Stochastic_ST_third_conv_binary\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h0usrCPKnCv7","colab_type":"text"},"source":["### Training:"]},{"cell_type":"code","metadata":{"id":"IDqk-VlfnEcs","colab_type":"code","colab":{}},"source":["# optimizer\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","train_loss, train_acc, val_loss, val_acc = training(use_gpu, model, name_model, nb_epoch, train_loader, test_loader,\n","                                                    optimizer, plot_result, slope_annealing)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"stExvUkXlGGo","colab_type":"text"},"source":["### Test:"]},{"cell_type":"code","metadata":{"id":"284OpQsOlHgp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":135},"outputId":"8c39368a-fcc6-42ba-b2ca-c8f38ce40da6","executionInfo":{"status":"ok","timestamp":1586767754480,"user_tz":-120,"elapsed":64104,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}}},"source":["# test\n","model.load_state_dict(load('trained_models/Omniglot_classif/' + name_model + '.pt', map_location=torch.device('cpu')))\n","test_loss, test_acc = test(use_gpu, model, test_loader)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Work/Thesis_Julien_Dejasmin/Work/code/Binary_activations_V2/MNIST_Binary_V2/utils/training.py:133: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(data, volatile=True), Variable(target)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["Test set: Average loss: 5.5008, Accuracy: 302/1928 (16%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pcWn5JMik7GF","colab_type":"text"},"source":["## Omniglot classification Network with binary fourth conv layer:"]},{"cell_type":"code","metadata":{"id":"pY3OS2W_kvdZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"aa4e991d-794f-47e5-f8b5-3c607f758e11","executionInfo":{"status":"ok","timestamp":1586767754484,"user_tz":-120,"elapsed":64096,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}}},"source":["model, name_model = binary_model_fourth()\n","print(name_model)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["no gpu available !\n","Omniglot_classif_Stochastic_ST_fourth_conv_binary\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q4eRgxP5nmtq","colab_type":"text"},"source":["### Training:"]},{"cell_type":"code","metadata":{"id":"V5r7yqeZnoTT","colab_type":"code","colab":{}},"source":["# optimizer\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","train_loss, train_acc, val_loss, val_acc = training(use_gpu, model, name_model, nb_epoch, train_loader, test_loader,\n","                                                    optimizer, plot_result, slope_annealing)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2w0bat1-lIO9","colab_type":"text"},"source":["### Test:"]},{"cell_type":"code","metadata":{"id":"MXtTZXdNkw0j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":135},"outputId":"8fc09616-6e71-4a3b-c7de-93854d0c1ae8","executionInfo":{"status":"ok","timestamp":1586767784358,"user_tz":-120,"elapsed":93958,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}}},"source":["# test\n","model.load_state_dict(load('trained_models/Omniglot_classif/' + name_model + '.pt', map_location=torch.device('cpu')))\n","test_loss, test_acc = test(use_gpu, model, test_loader)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Work/Thesis_Julien_Dejasmin/Work/code/Binary_activations_V2/MNIST_Binary_V2/utils/training.py:133: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(data, volatile=True), Variable(target)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["Test set: Average loss: 5.7963, Accuracy: 309/1928 (16%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gfppsgSC2b3N","colab_type":"text"},"source":["# Visualization:"]},{"cell_type":"markdown","metadata":{"id":"qw5PZds6lp0X","colab_type":"text"},"source":["## Visuazation regions that maximizes a specific layer and filter:"]},{"cell_type":"markdown","metadata":{"id":"vrAQKDZR3_DC","colab_type":"text"},"source":["### New method modules:"]},{"cell_type":"code","metadata":{"id":"uELKcmS3l9I1","colab_type":"code","colab":{}},"source":["def get_all_values_fm_of_model_for_all_dataset(model, train_loader):\n","\n","    # a dictionary that keeps saving the activations as they come\n","    activations = collections.defaultdict(list)\n","    \n","    for name, m in model.named_modules():\n","    if type(m)==nn.Conv2d:\n","      # partial to assign the layer name to each hook\n","      m.register_forward_hook(partial(save_activation, name))\n","    \n","    # forward pass through the full dataset\n","    for batch in train_loader:\n","      out = model((batch[0], 1.0))\n","\n","    # concatenate all the outputs we saved to get the the activations for each layer for the whole dataset\n","    activations = {name: torch.cat(outputs, 0) for name, outputs in activations.items()}\n","\n","    # just print out the sizes of the saved activations as a sanity check\n","    for k,v in activations.items():\n","      print (k, v.size())\n","\n","    # return values in np array\n","    layer = []\n","    for i in ragen(len(activations)):\n","      layer.append(activations['layer1'].detach().numpy())\n","\n","    return layer\n","\n","\n","def save_activation(name, mod, inp, out):\n","\tactivations[name].append(out.cpu())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rIgev29924wk","colab_type":"code","colab":{}},"source":["def get_index_x_y_max_activation_fm(feature_map):\n","    \"\"\"\n","    return a index x,y values list for the high left pixel of the region interest\n","    \"\"\"\n","    ind_x = []\n","    ind_y = []\n","    act_value = []\n","\n","    for j in range(feature_map.shape[0]):\n","      local_ind_x = []\n","      local_ind_y = []\n","      act_local = []\n","      for i in range(feature_map.shape[1]):\n","\n","        act = max(feature_map[j][i].min(), feature_map[j][i].max(), key=abs)\n","        act_local.append(act)\n","        # index x, y of max activation in conv layer 2\n","        local_ind_x.append(int((np.where(feature_map[j][i]==act)[0])[0]))\n","        local_ind_y.append(int((np.where(feature_map[j][i]==act)[1])[0]))\n","      act_value.append(np.asarray(act_local))\n","      ind_x.append(np.asarray(local_ind_x))\n","      ind_y.append(np.asarray(local_ind_y))\n","    return ind_x, ind_y, act_value\n","\n","\n","def get_regions_max(ind_x, ind_y, stride, padding, len_img_h, len_img_w, filter_size, image, nb_fm):\n","  \"\"\"\n","  return region of interest from index x,y\n","  \"\"\"\n","  regions = []\n","  for i, im in enumerate(image):\n","    # for each image\n","    print('treating image n {}/{}'.format(i, len(image)))\n","    image = im[0][0][0]\n","    local_regions = []\n","\n","    for j in range(nb_fm):\n","      # for each fm\n","      ind_x_im = ind_x[i][j]\n","      ind_y_im = ind_y[i][j]\n","\n","      # determine pixel high left of region of interest:\n","      index_col_hl = (ind_x_im * stride) - filter_size//2\n","      index_raw_hl = (ind_y_im * stride) - filter_size//2\n","      \n","      if index_col_hl < 0:\n","        index_col_hl = 0\n","        reduice_region_col_size = index_col_hl\n","      else:\n","        reduice_region_col_size = 0\n","      if index_raw_hl < 0:\n","        index_raw_hl = 0\n","        reduice_region_raw_size = index_raw_hl\n","      else:\n","        reduice_region_raw_size = 0\n","\n","      begin_col = index_col_hl\n","      end_col = index_col_hl + filter_size + reduice_region_col_size\n","      begin_raw = index_raw_hl\n","      end_raw = index_raw_hl + filter_size + reduice_region_raw_size\n","\n","      if end_col > len_img_w:\n","        end_col = len_img_w\n","      if end_raw > len_img_h:\n","        end_raw = len_img_h\n","\n","      local_regions.append(image[begin_raw:end_raw, begin_col:end_col].detach().numpy())\n","    regions.append(local_regions)\n","  return regions"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pzPkrxC2oAvY","colab_type":"text"},"source":["### Modules for extract only region of interest:"]},{"cell_type":"code","metadata":{"id":"w7pqhmm9oCVJ","colab_type":"code","colab":{}},"source":["def get_regions_interest(regions, activation, best, worst, viz_mean_img, viz_grid, percentage=None, list_filter = None):\n","  \"\"\"\n","  get regions of interest\n","  \"\"\"\n","  nb_filter = activation.shape[1]\n","\n","  if best==False and worst==False:\n","    assert percentage!=None, \"if don't choice best or worst value, you didn't choice a percentage value\"\n","  if best==True and worst==True:\n","    raise TypeError('choice only one value at True between best an worst')\n","\n","  # consider only regions of all image of list_filter or all filter\n","  if list_filter == None:\n","    print('Interest of all filters')\n","    regions_interest_filter = regions\n","    activations_values_interest = activation\n","    nb_filter = nb_filter\n","  else:\n","    assert max(list_filter) < nb_filter and min(list_filter) >= 0, 'filter choisen out of range'\n","    print('Interest of filters:', list_filter)\n","    regions_interest_filter = get_index_filter_interest(regions, list_filter)\n","    activations_values_interest = activation[:, list_filter]\n","    nb_filter = len(list_filter)\n","\n","  # consider a percent of best or worst activations:\n","  if percentage == None:\n","    print('Consider all image regions')\n","    selected_regions = regions_interest_filter\n","  else:\n","    assert percentage <= 100 and percentage >= 0, 'percentage value must be in 0 and 100'\n","    n = int((len(activation)*percentage)/100)\n","    print('Consider {}% image regions = {} images'.format(percentage, n))\n","    selected_regions = get_n_first_regions_index(best, worst, n, activations_values_interest, nb_filter, regions_interest_filter)\n","\n","  # visualization: one mean image or grid image:\n","  if viz_mean_img:\n","      nb_image = 1\n","      print('mean image:')\n","      for i, ind_filter in enumerate(list_filter):\n","        print('mean regions of {} regions more={} or worst={} active for filter number: {} :'.format(n, best, worst, ind_filter))\n","        mean_img = np.mean(selected_regions[i])\n","        viz_regions(nb_image, mean_img)\n","\n","  if viz_grid:\n","      nb_image = n\n","      print('grid image')\n","      for i, ind_filter in enumerate(list_filter):\n","        reg = []\n","        print('grid regions of {} regions more={} or worst={} active for filter number: {} :'.format(n, best, worst, ind_filter))\n","        for j in range(len(selected_regions[i])):\n","          reg.append(selected_regions[i][j])\n","        viz_regions(nb_image, np.asarray(reg))\n","\n","  return selected_regions\n","\n","\n","def viz_regions(nb_image, regions):\n","  \"\"\"\n","  visualize region of interest\n","  \"\"\"\n","  regions = torch.tensor(regions) \n","  regions = regions.reshape((nb_image,1,regions.shape[-2],regions.shape[-1]))\n","  visTensor(regions, ch=0, allkernels=False)\n","  plt.ioff()\n","  plt.show()\n","\n","\n","def get_n_first_regions_index(best, worst, n, activation, nb_filter, regions):\n","  \"\"\"\n","  select only regions that we want \n","  \"\"\"\n","  regions_selected = []\n","  if best:\n","    for i in range(nb_filter):\n","      ind_filter = (-activation[:, i]).argsort()[:n]\n","      regions_selected.append(regions[ind_filter, i])\n","    return regions_selected\n","\n","  elif worst:\n","    for i in range(nb_filter):\n","      ind_filter = activation[:, i].argsort()[:n]\n","      regions_selected.append(regions[ind_filter, i])\n","    return regions_selected\n","\n","  else:\n","    print('choice worst or best with bool True or False')\n","\n","\n","def get_index_filter_interest(regions, list_filter):\n","  \"\"\"\n","  extract only regions of the filter interest\n","  \"\"\"\n","  return regions[:, list_filter]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hi49IPYnoEI0","colab_type":"text"},"source":["### Extract and save regions of interest with new method:"]},{"cell_type":"markdown","metadata":{"id":"48t2Dsxe41om","colab_type":"text"},"source":["#### For no binary model:"]},{"cell_type":"code","metadata":{"id":"KF90m_iE48WN","colab_type":"code","colab":{}},"source":["model, name_model = no_binary_model()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ksuwywAVoFQZ","colab_type":"code","colab":{}},"source":["# get all values of fm in model\n","layer = get_all_values_fm_of_model_for_all_dataset(model, train_loader)\n","\n","layer_1_fm_Omniglot_no_Binary = layer[0]\n","layer_2_fm_Omniglot_no_Binary = layer[1]\n","layer_3_fm_Omniglot_no_Binary = layer[2]\n","layer_4_fm_Omniglot_no_Binary = layer[3]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTYM1A0DoGm3","colab_type":"code","colab":{}},"source":["# save fm value\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer1.npy', layer_1_fm_Omniglot_no_Binary)\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer2.npy', layer_2_fm_Omniglot_no_Binary)\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer3.npy', layer_3_fm_Omniglot_no_Binary)\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer4.npy', layer_4_fm_Omniglot_no_Binary)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lDQwPnLYoH1I","colab_type":"code","colab":{}},"source":["# get index x,y of pixel high left of all interest region\n","ind_x_layer1, ind_y_layer1, activations_values_layer1 = get_index_x_y_max_activation_fm(layer_1_fm_Omniglot_no_Binary)\n","ind_x_layer2, ind_y_layer2, activations_values_layer2 = get_index_x_y_max_activation_fm(layer_2_fm_Omniglot_no_Binary)\n","ind_x_layer3, ind_y_layer3, activations_values_layer3 = get_index_x_y_max_activation_fm(layer_3_fm_Omniglot_no_Binary)\n","ind_x_layer4, ind_y_layer4, activations_values_layer4 = get_index_x_y_max_activation_fm(layer_4_fm_Omniglot_no_Binary)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FZwPac4noIrd","colab_type":"code","colab":{}},"source":["# save index x,y values and activations values\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer1_ind_x.npy', ind_x_layer1)\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer2_ind_x.npy', ind_x_layer2)\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer3_ind_x.npy', ind_x_layer3)\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer4_ind_x.npy', ind_x_layer4)\n","\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer1_ind_y.npy', ind_y_layer1)\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer2_ind_y.npy', ind_y_layer2)\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer3_ind_y.npy', ind_y_layer3)\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer4_ind_y.npy', ind_y_layer4)\n","\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer1_activation_values.npy', activations_values_layer1)\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer2_activation_values.npy', activations_values_layer2)\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer3_activation_values.npy', activations_values_layer3)\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer4_activation_values.npy', activations_values_layer4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X6W435GOoKW2","colab_type":"code","colab":{}},"source":["# parameters of model ot extract regions of interest\n","stride = 1\n","padding = 1\n","len_img_h = 105\n","len_img_w = 105\n","filter_size = 3\n","image = train_loader\n","nb_fm = layer_1_fm_Omniglot_no_Binary.shape[1]\n","\n","# extract all regions that maximize all fm in model\n","max_regions_Omniglot_no_binary_layer1 = get_regions_max(ind_x_layer1, ind_y_layer1, stride, padding, len_img_h, len_img_w, filter_size, image, nb_fm)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMAIxBnpoLNr","colab_type":"code","colab":{}},"source":["# save all regions taht maximize fm model\n","np.save('results/Omniglot_results/regions/no_binary_Omniglot_regions_max_layer1.npy', max_regions_Omniglot_no_binary_layer1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bQ-Yj8Qd44tB","colab_type":"text"},"source":["#### For Binary model"]},{"cell_type":"code","metadata":{"id":"oTBE1VjN46p9","colab_type":"code","colab":{}},"source":["binary_model_first()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6WHyOe9l5D2d","colab_type":"code","colab":{}},"source":["# get all values of fm in model\n","layer = get_all_values_fm_of_model_for_all_dataset(model, train_loader)\n","\n","layer_1_fm_Omniglot_Binary = layer[0]\n","layer_2_fm_Omniglot_Binary = layer[1]\n","layer_3_fm_Omniglot_Binary = layer[2]\n","layer_4_fm_Omniglot_Binary = layer[3]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8kMUa0X5FoM","colab_type":"code","colab":{}},"source":["# save fm value\n","np.save('results/Omniglot_results/regions/binary_Omniglot_fm_layer1.npy', layer_1_fm_Omniglot_Binary)\n","np.save('results/Omniglot_results/regions/binary_Omniglot_fm_layer2.npy', layer_2_fm_Omniglot_Binary)\n","np.save('results/Omniglot_results/regions/binary_Omniglot_fm_layer3.npy', layer_3_fm_Omniglot_Binary)\n","np.save('results/Omniglot_results/regions/binary_Omniglot_fm_layer4.npy', layer_4_fm_Omniglot_Binary)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JkkiX3wL5M3F","colab_type":"code","colab":{}},"source":["# get index x,y of pixel high left of all interest region\n","ind_x_layer1, ind_y_layer1, activations_values_layer1 = get_index_x_y_max_activation_fm(layer_1_fm_Omniglot_Binary)\n","ind_x_layer2, ind_y_layer2, activations_values_layer2 = get_index_x_y_max_activation_fm(layer_2_fm_Omniglot_Binary)\n","ind_x_layer3, ind_y_layer3, activations_values_layer3 = get_index_x_y_max_activation_fm(layer_3_fm_Omniglot_Binary)\n","ind_x_layer4, ind_y_layer4, activations_values_layer4 = get_index_x_y_max_activation_fm(layer_4_fm_Omniglot_Binary)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tAG3jBrk5PUa","colab_type":"code","colab":{}},"source":["# save index x,y values and activations values\n","np.save('results/Omniglot_results/regions/binary_Omniglot_fm_layer1_ind_x.npy', ind_x_layer1)\n","np.save('results/Omniglot_results/regions/binary_Omniglot_fm_layer2_ind_x.npy', ind_x_layer2)\n","np.save('results/Omniglot_results/regions/binary_Omniglot_fm_layer3_ind_x.npy', ind_x_layer3)\n","np.save('results/Omniglot_results/regions/binary_Omniglot_fm_layer4_ind_x.npy', ind_x_layer4)\n","\n","np.save('results/Omniglot_results/regions/binary_Omniglot_fm_layer1_ind_y.npy', ind_y_layer1)\n","np.save('results/Omniglot_results/regions/binary_Omniglot_fm_layer2_ind_y.npy', ind_y_layer2)\n","np.save('results/Omniglot_results/regions/binary_Omniglot_fm_layer3_ind_y.npy', ind_y_layer3)\n","np.save('results/Omniglot_results/regions/binary_Omniglot_fm_layer4_ind_y.npy', ind_y_layer4)\n","\n","np.save('results/Omniglot_results/regions/binary_Omniglot_fm_layer1_activation_values.npy', activations_values_layer1)\n","np.save('results/Omniglot_results/regions/binary_Omniglot_fm_layer2_activation_values.npy', activations_values_layer2)\n","np.save('results/Omniglot_results/regions/binary_Omniglot_fm_layer3_activation_values.npy', activations_values_layer3)\n","np.save('results/Omniglot_results/regions/binary_Omniglot_fm_layer4_activation_values.npy', activations_values_layer4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8rtR2iTL5YMD","colab_type":"code","colab":{}},"source":["# parameters of model ot extract regions of interest\n","stride = 1\n","padding = 1\n","len_img_h = 105\n","len_img_w = 105\n","filter_size = 3\n","image = train_loader\n","nb_fm = layer_1_fm_Omniglot_Binary.shape[1]\n","\n","# extract all regions that maximize all fm in model\n","max_regions_Omniglot_binary_layer1 = get_regions_max(ind_x_layer1, ind_y_layer1, stride, padding, len_img_h, len_img_w, filter_size, image, nb_fm)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UFQr6uR25iEH","colab_type":"code","colab":{}},"source":["# save all regions taht maximize fm model\n","np.save('results/Omniglot_results/regions/binary_Omniglot_regions_max_layer1.npy', max_regions_Omniglot_binary_layer1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uKMF0hl0oQrE","colab_type":"text"},"source":["### Visualization region interest:"]},{"cell_type":"markdown","metadata":{"id":"_fhcRgZyoR40","colab_type":"text"},"source":["#### Load regions that maximize a layer:"]},{"cell_type":"code","metadata":{"id":"376-YphToOX1","colab_type":"code","colab":{}},"source":["# load no binary MNIST data:\n","max_regions_Omniglot_no_binary_layer1 = np.load('results/Omniglot_results/regions/no_binary_Omniglot_regions_max_layer1.npy', allow_pickle=True)\n","activations_values_Omniglot_no_binary_layer1 = np.load('results/Omniglot_results/regions/no_binary_Omniglot_fm_layer1_activation_values.npy', allow_pickle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GVjMrw7poTlX","colab_type":"code","colab":{}},"source":["# load binary MNIST data:\n","max_regions_Omniglot_binary_layer1 = np.load('results/Omniglot_results/regions/binary_Omniglot_regions_max_layer1.npy', allow_pickle=True)\n","activations_values_Omniglot_binary_layer1 = np.load('results/Omniglot_results/regions/binary_Omniglot_fm_layer1_activation_values.npy', allow_pickle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4Je8qzAoVMb","colab_type":"text"},"source":["#### Viz:"]},{"cell_type":"code","metadata":{"id":"q4nzZaXFoXQi","colab_type":"code","colab":{}},"source":["# parameters\n","list_filter_interest = [0,1,2,9,4]\n","best = True\n","worst = False\n","viz_mean_img = True\n","viz_grid = True\n","percentage = 0.5\n","\n","# regions and activation of interest\n","regions = max_regions_Omniglot_no_binary_layer1\n","activations = activations_values_Omniglot_no_binary_layer1\n","\n","# TODO: mean with different shape (5,5) and (5,4)\n","# run\n","selected_regions = get_regions_interest(regions, activations, best, worst, viz_mean_img, viz_grid, percentage, list_filter_interest)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pOTBQ7b5oaTd","colab_type":"code","colab":{}},"source":["# compare two regions of a specific layer and filter in both model (binary and no binary)\n","# that maximize a specific image \n","id_image = 100\n","id_filter = 10\n","\n","print('region that maximize filter {} for the image {} in no binary MNIST network in layer1'.format(id_filter, id_image))\n","test = max_regions_Omniglot_no_binary_layer1[id_image][id_filter]\n","plt.imshow(test, cmap='gray')\n","plt.show()\n","\n","print('region that  maximize filter {} forthe  image {} in binary MNIST network in layer1'.format(id_filter, id_image))\n","test = max_regions_Omniglot_binary_layer1[id_image][id_filter]\n","plt.imshow(test, cmap='gray')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e-3KplkyoYfh","colab_type":"text"},"source":["# filter that maximize activation in a certain layer and filter:"]},{"cell_type":"markdown","metadata":{"id":"MkXqWfZDoeK5","colab_type":"text"},"source":["## Modules:"]},{"cell_type":"code","metadata":{"id":"382glj-bofAr","colab_type":"code","colab":{}},"source":["class GradientAscent:\n","    \"\"\"Provides an interface for activation maximization via gradient descent.\n","    This class implements the gradient ascent algorithm in order to perform\n","    activation maximization with convolutional neural networks (CNN).\n","    `Activation maximization <https://pdfs.semanticscholar.org/65d9/94fb778a8d9e0f632659fb33a082949a50d3.pdf>`_\n","    is one form of feature visualization that allows us to visualize what CNN\n","    filters are \"looking for\", by applying each filter to an input image and\n","    updating the input image so as to maximize the activation of the filter of\n","    interest (i.e. treating it as a gradient ascent task with activation as the\n","    loss). The implementation is inspired by `this demo <https://blog.keras.io/category/demo.html>`_\n","    by Francois Chollet.\n","    Args:\n","        model: A neural network model from `torchvision.models\n","            <https://pytorch.org/docs/stable/torchvision/models.html>`_,\n","            typically without the fully-connected part of the network.\n","            e.g. torchvisions.alexnet(pretrained=True).features\n","        img_size (int, optional, default=224): The size of an input image to be\n","            optimized.\n","        lr (float, optional, default=1.): The step size (or learning rate) of\n","            the gradient ascent.\n","        use_gpu (bool, optional, default=False): Use GPU if set to True and\n","            `torch.cuda.is_available()`.\n","    \"\"\"\n","\n","    ####################\n","    # Public interface #\n","    ####################\n","\n","    def __init__(self, model, img_size=105, lr=1., use_gpu=False):\n","        self.model = model\n","        self._img_size = img_size\n","        self._lr = lr\n","        self._use_gpu = use_gpu\n","\n","        self.num_layers = len(list(self.model.named_children()))\n","        self.activation = None\n","        self.gradients = None\n","\n","        self.handlers = []\n","\n","        self.output = None\n","\n","    @property\n","    def lr(self):\n","        return self._lr\n","\n","    @lr.setter\n","    def lr(self, lr):\n","        self._lr = lr\n","\n","    @property\n","    def img_size(self):\n","        return self._img_size\n","\n","    @img_size.setter\n","    def img_size(self, img_size):\n","        self._img_size = img_size\n","\n","    @property\n","    def use_gpu(self):\n","        return self._use_gpu\n","\n","    @use_gpu.setter\n","    def use_gpu(self, use_gpu):\n","        self._use_gpu = use_gpu\n","\n","    def optimize(self, layer, filter_idx, input_=None, num_iter=30):\n","        \"\"\"Generates an image that maximally activates the target filter.\n","        Args:\n","            layer (torch.nn.modules.conv.Conv2d): The target Conv2d layer from\n","                which the filter to be chosen, based on `filter_idx`.\n","            filter_idx (int): The index of the target filter.\n","            num_iter (int, optional, default=30): The number of iteration for\n","                the gradient ascent operation.\n","        Returns:\n","            output (list of torch.Tensor): With dimentions\n","                :math:`(num_iter, C, H, W)`. The size of the image is\n","                determined by `img_size` attribute which defaults to 224.\n","        \"\"\"\n","\n","        # Validate the type of the layer\n","\n","        if type(layer) != nn.modules.conv.Conv2d:\n","            raise TypeError('The layer must be nn.modules.conv.Conv2d.')\n","\n","        # Validate filter index\n","\n","        num_total_filters = layer.out_channels\n","        self._validate_filter_idx(num_total_filters, filter_idx)\n","\n","        # Inisialize input (as noise) if not provided\n","\n","        if input_ is None:\n","            input_ = np.uint8(np.random.uniform(\n","                150, 180, (self._img_size, self._img_size, 1)))\n","            input_ = apply_transforms(input_, size=self._img_size)\n","\n","        if torch.cuda.is_available() and self.use_gpu:\n","            self.model = self.model.to('cuda')\n","            input_ = input_.to('cuda')\n","\n","        # Remove previous hooks if any\n","\n","        while len(self.handlers) > 0:\n","            self.handlers.pop().remove()\n","\n","        # Register hooks to record activation and gradients\n","\n","        self.handlers.append(self._register_forward_hooks(layer, filter_idx))\n","        self.handlers.append(self._register_backward_hooks())\n","\n","        # Inisialize gradients\n","\n","        self.gradients = torch.zeros(input_.shape)\n","\n","        # Optimize\n","\n","        return self._ascent(input_, num_iter)\n","\n","    def visualize(self, layer, filter_idxs=None, lr=1., num_iter=30,\n","                  num_subplots=4, figsize=(4, 4), title='Conv2d',\n","                  return_output=False):\n","        \"\"\"Optimizes for the target layer/filter and visualizes the output.\n","        A method that combines optimization and visualization. There are\n","        mainly 3 types of operations, given a target layer:\n","        1. If `filter_idxs` is provided as an integer, it optimizes for the\n","            filter specified and plots the output.\n","        2. If `filter_idxs` is provided as a list of integers, it optimizes for\n","            all the filters specified and plots the output.\n","        3. if `filter_idx` is not provided, i.e. None, it randomly chooses\n","            `num_subplots` number of filters from the layer provided and\n","            plots the output.\n","        It also returns the output of the optimization, if specified with\n","        `return_output=True`.\n","        Args:\n","            layer (torch.nn.modules.conv.Conv2d): The target Conv2d layer from\n","                which the filter to be chosen, based on `filter_idx`.\n","            filter_idxs (int or list of int, optional, default=None): The index\n","                or indecies of the target filter(s).\n","            lr (float, optional, default=.1): The step size of optimization.\n","            num_iter (int, optional, default=30): The number of iteration for\n","                the gradient ascent operation.\n","            num_subplots (int, optional, default=4): The number of filters to\n","                optimize for and visualize. Relevant in case 3 above.\n","            figsize (tuple, optional, default=(4, 4)): The size of the plot.\n","                Relevant in case 1 above.\n","            title (str, optional default='Conv2d'): The title of the plot.\n","            return_output (bool, optional, default=False): Returns the\n","                output(s) of optimization if set to True.\n","        Returns:\n","            For a single optimization (i.e. case 1 above):\n","                output (list of torch.Tensor): With dimentions\n","                    :math:`(num_iter, C, H, W)`. The size of the image is\n","                    determined by `img_size` attribute which defaults to 224.\n","            For multiple optimization (i.e. case 2 or 3 above):\n","                output (list of list of torch.Tensor): With dimentions\n","                    :math:`(num_subplots, num_iter, C, H, W)`. The size of the\n","                    image is determined by `img_size` attribute which defaults\n","                    to 224.\n","        \"\"\"\n","\n","        self._lr = lr\n","\n","        if (type(filter_idxs) == int):\n","            output = self._visualize_filter(layer,\n","                                            filter_idxs,\n","                                            num_iter=num_iter,\n","                                            figsize=figsize,\n","                                            title=title)\n","        else:\n","            num_total_filters = layer.out_channels\n","\n","            if filter_idxs is None:\n","                num_subplots = min(num_total_filters, num_subplots)\n","                filter_idxs = np.random.choice(range(num_total_filters),\n","                                               size=num_subplots)\n","\n","            self._visualize_filters(layer,\n","                                    filter_idxs,\n","                                    num_iter,\n","                                    len(filter_idxs),\n","                                    title=title)\n","\n","        if return_output:\n","            return self.output\n","\n","    #####################\n","    # Private interface #\n","    #####################\n","\n","    def _register_forward_hooks(self, layer, filter_idx):\n","        def _record_activation(module, input_, output):\n","            self.activation = torch.mean(output[:,filter_idx,:,:])\n","\n","        return layer.register_forward_hook(_record_activation)\n","\n","    def _register_backward_hooks(self):\n","        def _record_gradients(module, grad_in, grad_out):\n","            if self.gradients.shape == grad_in[0].shape:\n","                self.gradients = grad_in[0]\n","\n","        for _, module in self.model.named_modules():\n","            if isinstance(module, nn.modules.conv.Conv2d) and \\\n","                    module.in_channels == 1:\n","                return module.register_backward_hook(_record_gradients)\n","\n","    def _ascent(self, x, num_iter):\n","        output = []\n","\n","        for i in range(num_iter):\n","            self.model(x)\n","\n","            self.activation.backward()\n","\n","            self.gradients /= (torch.sqrt(torch.mean(\n","                torch.mul(self.gradients, self.gradients))) + 1e-5)\n","\n","            x = x + self.gradients * self._lr\n","            output.append(x)\n","\n","        return output\n","\n","    def _validate_filter_idx(self, num_filters, filter_idx):\n","        if not np.issubdtype(type(filter_idx), np.integer):\n","            raise TypeError('Indecies must be integers.')\n","        elif (filter_idx < 0) or (filter_idx > num_filters):\n","            raise ValueError(f'Filter index must be between 0 and {num_filters - 1}.')\n","\n","    def _visualize_filter(self, layer, filter_idx, num_iter, figsize, title):\n","        self.output = self.optimize(layer, filter_idx, num_iter=num_iter)\n","\n","        plt.figure(figsize=figsize)\n","        plt.axis('off')\n","        plt.title(title)\n","        \n","        plt.imshow(format_for_plotting(\n","            standardize_and_clip(self.output[-1],\n","                                 saturation=0.15,\n","                                 brightness=0.7)), cmap='gray');\n","        \n","        # plt.imshow(self.output[-1])\n","\n","    def _visualize_filters(self, layer, filter_idxs, num_iter, num_subplots,\n","                           title):\n","        # Prepare the main plot\n","\n","        num_cols = 4\n","        num_rows = int(np.ceil(num_subplots / num_cols))\n","\n","        fig = plt.figure(figsize=(16, num_rows * 5))\n","        plt.title(title)\n","        plt.axis('off')\n","\n","        self.output = []\n","\n","        # Plot subplots\n","\n","        for i, filter_idx in enumerate(filter_idxs):\n","            output = self.optimize(layer, filter_idx, num_iter=num_iter)\n","\n","            self.output.append(output)\n","\n","            ax = fig.add_subplot(num_rows, num_cols, i+1)\n","            ax.set_xticks([])\n","            ax.set_yticks([])\n","            ax.set_title(f'filter {filter_idx}')\n","\n","            \n","            ax.imshow(format_for_plotting(\n","                standardize_and_clip(output[-1],\n","                                     saturation=0.15,\n","                                     brightness=0.7)), cmap='gray')\n","            \n","            # ax.imshow(self.output[-1])\n","            \n","        plt.subplots_adjust(wspace=0, hspace=0);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YF_bJSrzohWE","colab_type":"code","colab":{}},"source":["def apply_transforms(image, size=105):\n","    \"\"\"Transforms a PIL image to torch.Tensor.\n","    Applies a series of tranformations on PIL image including a conversion\n","    to a tensor. The returned tensor has a shape of :math:`(N, C, H, W)` and\n","    is ready to be used as an input to neural networks.\n","    First the image is resized to 256, then cropped to 224. The `means` and\n","    `stds` for normalisation are taken from numbers used in ImageNet, as\n","    currently developing the package for visualizing pre-trained models.\n","    The plan is to to expand this to handle custom size/mean/std.\n","    Args:\n","        image (PIL.Image.Image or numpy array)\n","        size (int, optional, default=224): Desired size (width/height) of the\n","            output tensor\n","    Shape:\n","        Input: :math:`(C, H, W)` for numpy array\n","        Output: :math:`(N, C, H, W)`\n","    Returns:\n","        torch.Tensor (torch.float32): Transformed image tensor\n","    Note:\n","        Symbols used to describe dimensions:\n","            - N: number of images in a batch\n","            - C: number of channels\n","            - H: height of the image\n","            - W: width of the image\n","    \"\"\"\n","\n","    if not isinstance(image, Image.Image):\n","        image = F.to_pil_image(image)\n","\n","    # means = [0.485, 0.456, 0.406]\n","    # stds = [0.229, 0.224, 0.225]\n","    # to only one channel\n","    means = [0.406]\n","    stds = [0.225]\n","\n","    transform = transforms.Compose([\n","        transforms.Resize(size),\n","        transforms.CenterCrop(size),\n","        transforms.ToTensor(),\n","        transforms.Normalize(means, stds)\n","    ])\n","\n","    tensor = transform(image).unsqueeze(0)\n","\n","    tensor.requires_grad = True\n","\n","    return tensor"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hgsnjUlloiK0","colab_type":"code","colab":{}},"source":["def format_for_plotting(tensor):\n","    \"\"\"Formats the shape of tensor for plotting.\n","    Tensors typically have a shape of :math:`(N, C, H, W)` or :math:`(C, H, W)`\n","    which is not suitable for plotting as images. This function formats an\n","    input tensor :math:`(H, W, C)` for RGB and :math:`(H, W)` for mono-channel\n","    data.\n","    Args:\n","        tensor (torch.Tensor, torch.float32): Image tensor\n","    Shape:\n","        Input: :math:`(N, C, H, W)` or :math:`(C, H, W)`\n","        Output: :math:`(H, W, C)` or :math:`(H, W)`, respectively\n","    Return:\n","        torch.Tensor (torch.float32): Formatted image tensor (detached)\n","    Note:\n","        Symbols used to describe dimensions:\n","            - N: number of images in a batch\n","            - C: number of channels\n","            - H: height of the image\n","            - W: width of the image\n","    \"\"\"\n","\n","    has_batch_dimension = len(tensor.shape) == 4\n","    formatted = tensor.clone()\n","\n","    if has_batch_dimension:\n","        formatted = tensor.squeeze(0)\n","\n","    if formatted.shape[0] == 1:\n","        return formatted.squeeze(0).detach()\n","    else:\n","        return formatted.permute(1, 2, 0).detach()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pQgJIhn_ojSE","colab_type":"code","colab":{}},"source":["def standardize_and_clip(tensor, min_value=0.0, max_value=1.0,\n","                         saturation=0.1, brightness=0.5):\n","\n","    \"\"\"Standardizes and clips input tensor.\n","    Standardizes the input tensor (mean = 0.0, std = 1.0). The color saturation\n","    and brightness are adjusted, before tensor values are clipped to min/max\n","    (default: 0.0/1.0).\n","    Args:\n","        tensor (torch.Tensor):\n","        min_value (float, optional, default=0.0)\n","        max_value (float, optional, default=1.0)\n","        saturation (float, optional, default=0.1)\n","        brightness (float, optional, default=0.5)\n","    Shape:\n","        Input: :math:`(C, H, W)`\n","        Output: Same as the input\n","    Return:\n","        torch.Tensor (torch.float32): Normalised tensor with values between\n","            [min_value, max_value]\n","    \"\"\"\n","\n","    tensor = tensor.detach().cpu()\n","\n","    mean = tensor.mean()\n","    std = tensor.std()\n","\n","    if std == 0:\n","        std += 1e-7\n","\n","    standardized = tensor.sub(mean).div(std).mul(saturation)\n","    clipped = standardized.add(brightness).clamp(min_value, max_value)\n","\n","    return clipped\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tOvsmNpaogWr","colab_type":"text"},"source":["## No binary:"]},{"cell_type":"code","metadata":{"id":"8tXzN8p0ociC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"a2c1aa1f-95e5-4946-b0fe-634899ab087d","executionInfo":{"status":"ok","timestamp":1586771547770,"user_tz":-120,"elapsed":2222,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}}},"source":["model, name_model = no_binary_model()\n","print(name_model)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["use 1 gpu who named: Tesla T4\n","Omniglot_classif_NonBinaryNet\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cYJzY_xFonRO","colab_type":"code","colab":{}},"source":["g_ascent = GradientAscent(model)\n","g_ascent.use_gpu = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w1F3eyu_ooDc","colab_type":"code","colab":{}},"source":["conv1 = model.layer1\n","conv1_filters = [0,8,16,20,24,28,32,40,45,48,56,63]\n","\n","conv2 = model.layer2\n","conv2_filters = [0,8,16,20,24,28,32,40,45,48,56,63]\n","\n","conv3 = model.layer3\n","conv3_filters = [0,8,16,20,24,28,32,40,45,48,56,63]\n","\n","conv4 = model.layer4\n","conv4_filters = [0,8,16,20,24,28,32,40,45,48,56,63]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uJmOGFZsoo7g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Yvn7cvhG6tDRuwpaXpGtd3wZpvwM-gfu"},"outputId":"89bd51f0-5f99-45bd-b81c-439103f1380f","executionInfo":{"status":"ok","timestamp":1586771574283,"user_tz":-120,"elapsed":22781,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}}},"source":["g_ascent.visualize(conv1, conv1_filters, title='conv1');\n","g_ascent.visualize(conv2, conv2_filters, title='conv2');\n","g_ascent.visualize(conv3, conv3_filters, title='conv3');\n","g_ascent.visualize(conv4, conv4_filters, title='conv4');"],"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"Mucs92t37CCJ","colab_type":"text"},"source":["## Binary:"]},{"cell_type":"code","metadata":{"id":"ZJj-lco16mOg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"862c97ae-8905-4da9-83f0-a36c0d0a320d","executionInfo":{"status":"ok","timestamp":1586771575895,"user_tz":-120,"elapsed":1554,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}}},"source":["model, name_model = binary_model_first()\n","print(name_model)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["use 1 gpu who named: Tesla T4\n","Omniglot_classif_Stochastic_ST_first_conv_binary\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LaLEANbk8Tqd","colab_type":"code","colab":{}},"source":["g_ascent = GradientAscent(model)\n","g_ascent.use_gpu = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qaIgGg6t8VA7","colab_type":"code","colab":{}},"source":["conv1 = model.layer1\n","conv1_filters = [0,8,16,20,24,28,32,40,45,48,56,63]\n","\n","conv2 = model.layer2\n","conv2_filters = [0,8,16,20,24,28,32,40,45,48,56,63]\n","\n","conv3 = model.layer3\n","conv3_filters = [0,8,16,20,24,28,32,40,45,48,56,63]\n","\n","conv4 = model.layer4\n","conv4_filters = [0,8,16,20,24,28,32,40,45,48,56,63]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3iBmncsZ8XPK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1PP248aEsHGyjVvp1TWpSlYYDYzskyRB9"},"outputId":"5f2354b5-fae4-4460-e441-710b18e0ea31","executionInfo":{"status":"ok","timestamp":1586771598148,"user_tz":-120,"elapsed":23775,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}}},"source":["g_ascent.visualize(conv1, conv1_filters, title='conv1');\n","g_ascent.visualize(conv2, conv2_filters, title='conv2');\n","g_ascent.visualize(conv3, conv3_filters, title='conv3');\n","g_ascent.visualize(conv4, conv4_filters, title='conv4');"],"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"f33MOSjk8foA","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}